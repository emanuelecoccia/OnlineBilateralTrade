import numpy as np
import pandas as pd

class BaseEnvironment:
    def __init__(self)->None:
        self.valuation_sequence = NotImplemented
        raise NotImplementedError
    
    def get_valuations(self)->np.ndarray[float, float]:
        raise NotImplementedError

class SimpleBilateralEnvironment(BaseEnvironment):
    def __init__(self, T:int)->None:
        """
        This class is a dummy environment that generates a sequence of valuations.
        The sequence is generated by sampling at random points (0, 1/4), (3/4, 1) from a bernoulli distribution.
        """
        self.T:int = T
        # Determine the valuation sequence beforehand
        self.valuation_sequence:np.ndarray = self.construct_valuation_sequence()

    def construct_valuation_sequence(self)->np.ndarray:
        # Sample at random points (0, 1/4), (3/4, 1) from a bernoulli distribution
        left_point:np.ndarray = np.array([0, 1/4])
        right_point:np.ndarray = np.array([3/4, 1])
        points:np.ndarray = np.vstack([left_point, right_point])
        valuation_sequence_indices:np.ndarray[int] = np.random.choice(2, size=self.T)
        valuation_sequence:np.ndarray = points[valuation_sequence_indices]
        return valuation_sequence
    
    def get_valuations(self, index)->np.ndarray:
        # Return the valuations corresponding to the index
        return self.valuation_sequence[index]
    
    def get_best_expert(self)->tuple[tuple[float, float], float]:
        """
        This function computes the aggregated reward on the unit square and returns 
        the price pair under which all prices maximize the reward function.

        Complexity: O(T^2).
        """
        # Work with pandas for easier sorting
        df = pd.DataFrame(self.valuation_sequence, columns=['s', 'b'])

        # Sort the price pairs by b in descending order and s in ascending order
        sorted_df = df.sort_values(by=['b', 's'], ascending=[False, True]) # might be redundant

        # Eliminate all the price pairs with b <= s
        filtered_df = sorted_df[sorted_df['b'] >= sorted_df['s']]

        # Group by b
        grouped = filtered_df.groupby('b')

        # Get sorted group keys in descending order
        sorted_keys = sorted(grouped.groups.keys(), reverse=True)

        prev_curr_rewards:list[tuple[float, float]] = []
        max_reward:float = 0
        max_reward_tuple:tuple[float, float] = (0, 0)

        for b in sorted_keys:
            group = grouped.get_group(b)
            reward_index:int = 0
            cumulative_reward:float = 0 
            reward_max_index:int = len(prev_curr_rewards) - 1 
            # contains (s, reward). The first element is a fake reward at s=0
            curr_rewards:list[tuple[float, float]] = [(0, 0)] 

            # Iterate through the s values
            for s in group['s']:
                # Swipe through prev_s < s
                # Keep in mind that "prev" here refers to the previous b value
                while prev_curr_rewards and reward_index <= reward_max_index:
                    s_prev, reward_prev = prev_curr_rewards[reward_index]
                    if s_prev < s:
                        new_reward = reward_prev + cumulative_reward
                        # Append the curr_rewards before s
                        curr_rewards.append((s_prev, new_reward))
                        # Update index
                        reward_index += 1
                        # Update max reward
                        if new_reward > max_reward:
                            max_reward = new_reward
                            max_reward_tuple = (s_prev, b)
                    elif s == s_prev:
                        # Add the delta in reward of the s values associated with the previous b
                        if reward_index > 0:
                            cumulative_reward += reward_prev - prev_curr_rewards[reward_index-1][1]
                        else:
                            cumulative_reward += reward_prev
                        # Update index
                        reward_index += 1
                    else:
                        break

                # Increment the baseline reward with the current s
                cumulative_reward += b-s
                # If the current s is the same as the previous s, update the reward directly in the list
                if curr_rewards and s == curr_rewards[-1][0]:
                    reward_s = cumulative_reward
                    curr_rewards[-1] = (s, reward_s)
                else: # Calculate the reward
                    if prev_curr_rewards:
                        # Add the reward baseline to the previous reward (hence the need of the fake reward at s=0)
                        reward_s = cumulative_reward + prev_curr_rewards[min(reward_index, reward_max_index)][1]
                    else:
                        reward_s = cumulative_reward
                    curr_rewards.append((s, reward_s))
                # Update max reward
                if reward_s > max_reward:
                    max_reward = reward_s
                    max_reward_tuple = (s, b)

            # Swipe through prev_s > s if prev_s < b
            while prev_curr_rewards and reward_index <= reward_max_index:
                s_prev, reward_prev = prev_curr_rewards[reward_index]
                if reward_index <= reward_max_index and s_prev <= b:
                    # Append s_prev to vector of rewards
                    new_reward = reward_prev + cumulative_reward
                    curr_rewards.append((s_prev, new_reward))
                    # Update index
                    reward_index += 1
                    # Update max reward
                    if new_reward > max_reward:
                        max_reward = new_reward
                        max_reward_tuple = (s_prev, b)
                else:
                    break
            
            prev_curr_rewards = curr_rewards

        return max_reward_tuple, max_reward